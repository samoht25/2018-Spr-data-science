{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial Versus Parallel Processing\n",
    "\n",
    "All of the computer programming that we have done this course to this point has been serial, that is, in series. In serial processing, each operation must wait for the previous operation to complete before it begins. In parallel processing, each cpu in a system can carry out a separate process simultaneously. This means that one operation that does not depend on another does not need to wait for the previous operation to complete.\n",
    "\n",
    "#### The Sum of a Vector\n",
    "\n",
    "![](../doc/img/reduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce\n",
    "\n",
    "MapReduce is a programming model designed to make parallel programming easier. The paradigm consists of two separate processes, a **mapper** and a **reducer**.\n",
    "\n",
    "The mapper turns a document into a collection of key-value pairs. \n",
    "\n",
    "The reducer performs a specified operation over matching key-value pairs.\n",
    "\n",
    "We have seen something similar before when working with Mongo aggregate functions. Consider the following Mongo aggregate function:\n",
    "\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': '$word', \n",
    "            'count': { '$sum': 1 }\n",
    "            }\n",
    "    }\n",
    "\n",
    "Here, we are grouping documents and then counting them by adding one to the count for each document. When we did this, we did not worry about the implementation. We simply told Mongo what we wanted for the output and let Mongo figure out how to get there. Using MapReduce we will begin to Add a bit more detail in terms of how we want a certain long-running process executed.\n",
    "\n",
    "Let's think about how we might achieve the same end using the MapReduce paradigm. Note that we will be writing this in pure Python but typically this will be executed by a much larger and more powerful system such as Hive or Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can separate the above processed into two steps: \n",
    "\n",
    "1. the mapper that maps a document to a collection of word tokens\n",
    "1. the reducer that reduces a collection of word tokens by adding one to the count associated with each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "\n",
    "The use the following list of strings as our \"documents\". This is to say that each string is a document that will be mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This paper presents a kernel-based principal component analysis, kernel PCA, to extract critical features for improving the performance of a stock trading model. \",\n",
    "\"The feature extraction method is one of the techniques to solve dimensionality reduction problems.\",\n",
    "\"The kernel PCA is a feature extraction approach which has been applied to data transformation from known variables to capture critical information.\",\n",
    "\"The kernel PCA is a kernel-based data mapping tool that has characteristics of both principal component analysis and non-linear mapping.\",\n",
    "\"The feature selection method is another DRP technique that selects only a small set of features from known variables, but these features still indicate possible collinearity problems that fail to reflect clear information.\",\n",
    "\"However, most feature extraction methods use a variable mapping application to eliminate noisy and collinear variables. In this research, we use the kernel-PCA method in a stock trading model to transform stock technical indices which allows features of smaller dimension to be formed.\",\n",
    "\"The kernel-PCA method has been applied to various stocks and sliding window testing methods using both half-year and 1-year testing strategies. The experimental results show that the proposed method generates more profits than other DRP methods on the America stock market.\",\n",
    "\"This stock trading model is very practical for real-world application, and it can be implemented in a real-time environment.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, one document looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper presents a kernel-based principal component analysis, kernel PCA, to extract critical features for improving the performance of a stock trading model. '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokens\n",
    "\n",
    "Each token we generate will have the following form\n",
    "\n",
    "    (word, 1)\n",
    "    \n",
    "In Python, we will represent this using a tuple consisting of a word string and the number 1, e.g.\n",
    "\n",
    "    ('kernel-based', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper\n",
    "\n",
    "A complete mapper will return a list of properly formatted word tokens. Additionally, we need to have all punctuation removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(document):\n",
    "    punctuation = [\"'\", '\"', '`', '~', '!', '@', '#', '$', '%',\n",
    "                   '^', '&', '*', '(', ')', '-', '_', '+', '=',\n",
    "                   '{', '}', '[', ']', '|', '\\\\', ':', ';', ',',\n",
    "                   '<', '>', '.', '/', '?']\n",
    "    \n",
    "    for symbol in punctuation:\n",
    "        document = document.replace(symbol, '')\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper presents a kernelbased principal component analysis kernel PCA to extract critical features for improving the performance of a stock trading model '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we built the mapper we use this `remove_punctuation` function and then create a list of words using the `.split()` method available to the `list` class. Note that the default argument for `.split()` is to split on whitespace. We have also used the `yield` argument to create a generator for lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(document):\n",
    "    document = remove_punctuation(document)\n",
    "    words = document.split()\n",
    "    for word in words:\n",
    "        yield (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_0 = mapper(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This', 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(mapper_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('paper', 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(mapper_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('presents', 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(mapper_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(mapper_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we will pass these tokens to redis as they are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis import Redis \n",
    "\n",
    "redis_connection = Redis('this_redis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `rpush` or \"right push\" to push each token to a Redis list using the word as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(document, redis_connection):\n",
    "    document = remove_punctuation(document)\n",
    "    words = document.split()\n",
    "    for word in words:\n",
    "        token = 1\n",
    "        redis_connection.rpush(word, token)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = mapper(documents[0], redis_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will use the `lpop` or \"left pop\" to pop each token from the front the left of the Redis list. Note that this is a FIFO or \"first in, first out\" pattern, that is, a queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'component'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redis_connection.lpop('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'paper',\n",
       " 'presents',\n",
       " 'a',\n",
       " 'kernelbased',\n",
       " 'principal',\n",
       " 'component',\n",
       " 'analysis',\n",
       " 'kernel',\n",
       " 'PCA',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'critical',\n",
       " 'features',\n",
       " 'for',\n",
       " 'improving',\n",
       " 'the',\n",
       " 'performance',\n",
       " 'of',\n",
       " 'a',\n",
       " 'stock',\n",
       " 'trading',\n",
       " 'model']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer\n",
    "\n",
    "The reducer will operate in two steps:\n",
    "\n",
    "1. It will pop each tokens for a given word into memory\n",
    "2. It will reduce the list of tokens in Redis according to the operation we defined, in this case adding the second value from each token to the current count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(word, redis_connection):\n",
    "    count = 0\n",
    "    token = redis_connection.lpop(word)\n",
    "    while token:\n",
    "        token = int(token.decode())\n",
    "        count += token\n",
    "        token = redis_connection.lpop(word)\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = mapper(documents[0], redis_connection)\n",
    "words = mapper(documents[0], redis_connection)\n",
    "words = mapper(documents[0], redis_connection)\n",
    "words = mapper(documents[0], redis_connection)\n",
    "words = mapper(documents[0], redis_connection)\n",
    "reducer('component', redis_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word List\n",
    "\n",
    "The last thing to keep track of in this operation is a list of all the words seen. To do this we will use the `set` datatype also available in Redis via `.sadd()` and `.spop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(document, redis_connection, word_list):\n",
    "    document = remove_punctuation(document)\n",
    "    words = document.split()\n",
    "    for word in words:\n",
    "        token = 1\n",
    "        redis_connection.rpush(word, token)\n",
    "        redis_connection.sadd(word_list, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count via MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(documents, redis_connection, word_list='word_list'):\n",
    "\n",
    "    counts = []\n",
    "\n",
    "    for document in documents:\n",
    "        mapper(document, redis_connection, word_list)\n",
    "\n",
    "    word = redis_connection.spop(word_list)\n",
    "    while word:\n",
    "        word = word.decode()\n",
    "        count = reducer(word, redis_connection)\n",
    "        counts.append((word, count))\n",
    "        word = redis_connection.spop(word_list)\n",
    "        \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_test = [\"data science\", \"big data\", \"science fiction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 2), ('big', 1), ('fiction', 1), ('data', 2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count(documents_test, redis_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('component', 2),\n",
       " ('performance', 7),\n",
       " ('solve', 1),\n",
       " ('transform', 1),\n",
       " ('presents', 7),\n",
       " ('various', 1),\n",
       " ('In', 1),\n",
       " ('than', 1),\n",
       " ('halfyear', 1),\n",
       " ('using', 1),\n",
       " ('kernelPCA', 2),\n",
       " ('results', 1),\n",
       " ('extraction', 3),\n",
       " ('tool', 1),\n",
       " ('technical', 1),\n",
       " ('strategies', 1),\n",
       " ('variable', 1),\n",
       " ('kernelbased', 8),\n",
       " ('known', 2),\n",
       " ('realworld', 1),\n",
       " ('it', 1),\n",
       " ('still', 1),\n",
       " ('has', 3),\n",
       " ('reduction', 1),\n",
       " ('trading', 9),\n",
       " ('variables', 3),\n",
       " ('sliding', 1),\n",
       " ('principal', 8),\n",
       " ('kernel', 9),\n",
       " ('window', 1),\n",
       " ('other', 1),\n",
       " ('on', 1),\n",
       " ('stocks', 1),\n",
       " ('feature', 4),\n",
       " ('DRP', 2),\n",
       " ('testing', 2),\n",
       " ('been', 2),\n",
       " ('of', 11),\n",
       " ('profits', 1),\n",
       " ('problems', 2),\n",
       " ('and', 5),\n",
       " ('which', 2),\n",
       " ('environment', 1),\n",
       " ('in', 2),\n",
       " ('This', 8),\n",
       " ('market', 1),\n",
       " ('is', 5),\n",
       " ('techniques', 1),\n",
       " ('paper', 6),\n",
       " ('critical', 8),\n",
       " ('to', 15),\n",
       " ('nonlinear', 1),\n",
       " ('1year', 1),\n",
       " ('data', 2),\n",
       " ('the', 11),\n",
       " ('improving', 7),\n",
       " ('but', 1),\n",
       " ('a', 20),\n",
       " ('small', 1),\n",
       " ('realtime', 1),\n",
       " ('extract', 7),\n",
       " ('methods', 3),\n",
       " ('practical', 1),\n",
       " ('application', 2),\n",
       " ('clear', 1),\n",
       " ('set', 1),\n",
       " ('generates', 1),\n",
       " ('show', 1),\n",
       " ('noisy', 1),\n",
       " ('analysis', 8),\n",
       " ('mapping', 3),\n",
       " ('possible', 1),\n",
       " ('transformation', 1),\n",
       " ('can', 1),\n",
       " ('experimental', 1),\n",
       " ('selects', 1),\n",
       " ('selection', 1),\n",
       " ('implemented', 1),\n",
       " ('be', 2),\n",
       " ('smaller', 1),\n",
       " ('indices', 1),\n",
       " ('capture', 1),\n",
       " ('stock', 11),\n",
       " ('use', 2),\n",
       " ('indicate', 1),\n",
       " ('reflect', 1),\n",
       " ('dimensionality', 1),\n",
       " ('features', 10),\n",
       " ('approach', 1),\n",
       " ('PCA', 9),\n",
       " ('allows', 1),\n",
       " ('more', 1),\n",
       " ('both', 2),\n",
       " ('eliminate', 1),\n",
       " ('collinear', 1),\n",
       " ('method', 5),\n",
       " ('very', 1),\n",
       " ('another', 1),\n",
       " ('proposed', 1),\n",
       " ('The', 6),\n",
       " ('only', 1),\n",
       " ('dimension', 1),\n",
       " ('America', 1),\n",
       " ('characteristics', 1),\n",
       " ('collinearity', 1),\n",
       " ('formed', 1),\n",
       " ('technique', 1),\n",
       " ('fail', 1),\n",
       " ('information', 2),\n",
       " ('these', 1),\n",
       " ('from', 2),\n",
       " ('this', 1),\n",
       " ('research', 1),\n",
       " ('applied', 2),\n",
       " ('However', 1),\n",
       " ('we', 1),\n",
       " ('one', 1),\n",
       " ('most', 1),\n",
       " ('that', 4),\n",
       " ('for', 8),\n",
       " ('model', 9)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count(documents, redis_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Where can this operation be parallelized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
